\documentclass[fleqn,a4paper,12pt]{article}
\usepackage[top=1in, bottom=1in, left=1in, right=1in]{geometry}



\title{Machine Learning Technique Homework 3}
\date{}

\setcounter{section}{3}

\usepackage{listings}

\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{float}


\usepackage{mathspec}
\setmainfont{Noto Serif CJK TC}
% \setmathsfont(Digits,Latin,Greek)[Numbers={Lining,Proportional}]{DejaVu Math TeX Gyre}
\newfontfamily\ZhFont{Noto Serif CJK TC}
\newfontfamily\SmallFont[Scale=0.8]{Droid Sans}
% \newfontfamily\SmallSmallFont[Scale=0.7]{Noto Serif CJK}
\usepackage{fancyhdr}
\usepackage{lastpage}
\pagestyle{fancy}
\fancyhf{}
\rhead{B03902072\ZhFont{江廷睿}}
\lhead{Machine Learning Technique Homework 3}
\rfoot{\thepage / \pageref{LastPage}}



\begin{document}
\maketitle
\thispagestyle{fancy}

\section*{1}

Let

$$(\tilde{x}_n, \tilde{y}_n) = (\sqrt{u_n}y_n, \sqrt{u_n}x_n)$$

then

\begin{align*}
  \min_{w} E^u_{\in}(w)
  &= \frac{1}{N} \sum_{n=1}^N u_n(y_n - w^Tx_n)^2 \\
  &= \frac{1}{N} \sum_{n=1}^N (\sqrt{u_n}y_n - w^T \sqrt{u_n} x_n)^2 \\
  &= \frac{1}{N} \sum_{n=1}^N (\tilde{y}_n - w^T \tilde{x}_n)^2
\end{align*}

Thus they are equivalent.


\section*{2}

All positive examples are classified correctly in the first iteration, while all negative examples are classified incorrectly. And we know that the examples should be reweighted so that the weighted number of correct examples is equal to number of incorrect examples. Therefore,

$$u^{(2)}_{+}  \text{number of positive example} =  u^{(2)}_{-} \text{number of negative example}$$
\begin{align*}
  u^{(2)}_{+} / u^{(2)}_{-}
  =&   \text{number of negative example} / \text{number of positive example} \\
  =& \frac{1}{99}
\end{align*}


\section*{3}

For each dimension, consider $\theta$ between $(0, 1), (1, 2), (2, 3), \cdots, (5, 6)$ with $s = {+1, -1}$, totally $12$ different decision stumps. And there are two dimension; therefore, there are $24$ decision stumps.

\section*{4}

\begin{align*}
  K_{ds}(x, x')
  =& (\phi_{ds}(x))^T(\phi_{ds}(x')) \\
  =& g_1(x)g_1(x') + g_2(x)g_2(x') + \cdots + g_{|\mathcal{G}|}(x)g_{|\mathcal{G}|}(x')
\end{align*}

Let $g_n(x) = s_n \cdot \mathrm{sign}(x_{i_n} - \theta_n)$

\begin{align*}
  g_n(x)g_n(x')
  &= s_n \cdot \mathrm{sign}(x_{i_n} - \theta_n) s_n \cdot \mathrm{sign}(x'_{i_n} - \theta_n) \\
  &= s_n^2 \cdot \mathrm{sign}(x_{i_n} - \theta_n) \mathrm{sign}(x'_{i_n} - \theta_n) \\  
  &= \mathrm{sign}(x_{i_n} - \theta_n) \mathrm{sign}(x'_{i_n} - \theta_n) \\
  &=\begin{cases}
    1 & \mathrm{sign}(x_{i_n} - \theta_n) = \mathrm{sign}(x'_{i_n} - \theta_n) \\
    -1 & \mathrm{sign}(x_{i_n} - \theta_n) \ne \mathrm{sign}(x'_{i_n} - \theta_n) \\
  \end{cases}
\end{align*}

Therefore, 

\begin{align*}
  g_n(x)g_n(x')
  =& (+1) \cdot |\{g | g(x) = g(x')\}| + (-1) \cdot |\{g | g(x) \ne g(x')\}| \\
  =& (+1) \cdot (2(R - L + 1) - |x_1 - x_1'| - |x_2 - x_2'|) + \\
    & (-1) \cdot (|x_1 - x_1'| + |x_2 - x_2'|)
\end{align*}

where the second equality is because $g_n(x) \ne g_n(x')$ only when $\theta_n$ is between $(x_i, x_i + 1), (x_i + 1, x_i + 2), \cdots, (x'_i - 1, x'_i)$, totally $|x'_i - x_i|$ decision stumps, and trivially, $|\{g | g(x) = g(x')\}| = |\text{decision stumps}| - |\{g | g(x) \ne g(x')\}|$.

To work with continuous input vector, we can first sort $x_1, x_2$ respectively. Then we can replace $(x_1, x_2)$ with $(d_1, d_2)$, where $d_1, d_2$ is the indice of $x_1, x_2$ in the sorted $x_1, x_2$. So 

\begin{equation*}
  g_n(x)g_n(x') = (-1) \cdot (|d_1 - d_1'| + |d_2 - d_2'|)
\end{equation*}



\section*{5}

\begin{align*}
  1 - \mu_+^2 - \mu_-^2
  &= 1 - \mu_+^2 - (1 - \mu_+^2) \\
  &= 1 - 2 \mu_+^2 + 2 \mu_+  - 1 \\
  & -2 \mu_+^2 + 2 \mu_+
\end{align*}

it gets the maximum value $\frac{1}{2}$ when $\mu_+ = \mu_- = \frac{1}{2}$.

\section*{6}

The answer is \textbf{[e]}.\\

Since $\max_{\mu_+, \mu_-} 1 - \mu_+^2 - \mu_-^2 = \frac{1}{2}$, it is normalized to

\begin{equation*}
  \tilde{f}_0(\mu_+, \mu_-) = 2(1 - \mu_+^2 - \mu_i^2) = 2(1 - \mu_+^2 - (1 - \mu_+)^2) = 4 \mu_+ - 4 \mu_+^2 
\end{equation*}

\subsection*{[a]}

\begin{equation*}
  \left. \min(\mu_+, \mu_-) \right|_{\mu+=\frac{1}{4}, \mu_-=\frac{1}{4}} = \frac{1}{4} \ne \frac{3}{4} = \tilde{f}_0(\frac{1}{4}, \frac{3}{4})
\end{equation*}

thus not [a].

\subsection*{[b]}

Let

\begin{equation*}
  f_b(\mu_+, \mu_-) = \mu_+(1 - (\mu_+ - \mu_-))^2 + \mu_-(-1 - (\mu_+ - \mu_-))^2
\end{equation*}

\begin{align*}
  &  \frac{\partial}{\partial \mu_+} \mu_+(1 - (\mu_+ - \mu_-))^2 + \mu_-(-1 - (\mu_+ - \mu_-))^2 \\
  =& \frac{\partial}{\partial \mu_+} \mu_+(1 - (\mu_+ - (1 - \mu_+)))^2 + (1 - \mu_+)(-1 - (\mu_+ - (1 - \mu_+)))^2 \\
  =& \frac{\partial}{\partial \mu_+}  4 \mu_+^2 (1 - \mu_+) \\  
  =& 4 (2\mu_+(1 - \mu_+) - \mu_+^2 ) \\
  =& 4 (2\mu_+ - 3 \mu_+^2 ) = 0 \text{ when $\mu_+ = \frac{2}{3}$ or  $\mu_+ = 0$.}
\end{align*}

Since $f_b(0, 1) = f_b(1, 0) = 0 < f_b(\frac{2}{3}, \frac{1}{3}) = \frac{16}{27}$,

\begin{equation*}
  \arg \max f_b(\mu_+, \mu_-) = (\frac{2}{3}, \frac{1}{3})
\end{equation*}

Thus the normalized $f_b$ is

\begin{equation*}
  \tilde{f}_b(\mu_+, \mu_-) = \frac{27}{16} (\mu_+(1 - (\mu_+ - \mu_-))^2 + \mu_-(-1 - (\mu_+ - \mu_-))^2)
\end{equation*}

But

\begin{equation*}
  \tilde{f}_b(\frac{1}{2}, \frac{1}{2}) = \frac{27}{32} \ne 1 = \tilde{f}_0(\frac{1}{2}, \frac{1}{2})
\end{equation*}

thus not [b].

\subsection*{[c]}

Let

\begin{align*}
  f_c(\mu_+, \mu_-) =
  &  - \mu_+ \ln \mu_+ - \mu_- \ln \mu_- \\
  =& - \mu_+ \ln \mu_+ - (1 - \mu_+) \ln (1 - \mu_+) \\
\end{align*}

\begin{align*}
  &\frac{\partial}{\partial \mu_+} f_c(\mu_+, \mu_-) \\
  &=  - \ln \mu_+ - 1 +  \ln (1 - \mu_+) + 1 = 0 \text{ when $\mu_+ = \frac{1}{2}$.}
\end{align*}

Thus

\begin{equation*}
  \max f_c(\mu_+, \mu_-) = \ln 2
\end{equation*}

and the normalized $f_c$ is

\begin{equation*}
  \tilde{f}_c(\mu_+, \mu_-) = - \mu_+ \log_2 \mu_+ - \mu_- \log_2 \mu_- \\  
\end{equation*}

But 

\begin{align*}
  \tilde{f}_c(\frac{3}{4}, \frac{1}{4})
  &= - \frac{3}{4} \log_2 \frac{3}{4} - \frac{1}{4} \log_2 \frac{1}{4} \\
  &= 2 -\frac{3}{4} \log_2 3 \ne \frac{3}{4} = \tilde{f_0}(\frac{3}{4}, \frac{1}{4})
\end{align*}

thus not [c].

\subsection*{[d]}

Let 

\begin{equation*}
  f_d(\mu_+, \mu_-) = 1 - |\mu_+ - \mu_-|
\end{equation*}

\begin{equation*}
  \max f_d(\mu_+, \mu_-) = 1
\end{equation*}

thus nromalized $f_d$ is 

\begin{equation*}
  \tilde{f}_d(\mu_+, \mu_-) = f_d(\mu_+, \mu_-) = 1 - |\mu_+ - \mu_-|
\end{equation*}

But 

\begin{equation*}
  \tilde{f}_d(\frac{3}{4}, \frac{1}{4}) = \frac{1}{2} \ne \frac{3}{4} = \tilde{f}_0
\end{equation*}

Thus not [d].

\section*{7}

$E_{in}(g_t) = 0.24$, $\alpha_1 = 0.5763397549691922$

\begin{figure}[h]
\centering
\includegraphics[width=.6\linewidth]{ada-ein-g.png}
\caption{$t$ versus $E_{in}(g_t)$}
\label{fig:ada-ein-g}
\end{figure}

\section*{8}

It is neither increasing nor decreasing. In just oscillates severely. The reason may be the changing of sample weights during the training process.

\section*{9}

\begin{figure}[H]
\centering
\includegraphics[width=.6\linewidth]{ada-ein.png}
\caption{$t$ versus $E_{in}(G_t)$}
\label{fig:ada-ein-train}
\end{figure}


\section*{10}

\begin{figure}[H]
\centering
\includegraphics[width=.6\linewidth]{ada-ut.png}
\caption{$t$ versus $U_t$}
\label{fig:ada-ut}
\end{figure}

$U_2 = 65.450396, U_T = 0.540149$.

\section*{11}

\begin{figure}[H]
\centering
\includegraphics[width=.6\linewidth]{ada-epsilon.png}
\caption{$t$ versus $\epsilon_t$}
\label{fig:ada-ut}
\end{figure}

The minimum $\epsilon = 0.178728070175$


\section*{12}

\begin{figure}[H]
\centering
\includegraphics[width=.6\linewidth]{ada-eout-g.png}
\caption{$t$ versus $E_{out}(g_t)$}
\label{fig:ada-eout-g}
\end{figure}

$E_{out}(G_1) = 0.29$

\section*{13}

\begin{figure}[H]
\centering
\includegraphics[width=.6\linewidth]{ada-eout.png}
\caption{$t$ versus $E_{out}(G_t)$}
\label{fig:ada-eout}
\end{figure}

$E_{out}(G) = 0.132$

\section*{14}

\begin{figure}[H]
\centering
\includegraphics[width=\linewidth]{tree.png}
\caption{Decision Tree}
\label{fig:tree}
\end{figure}

Note: left path implies if the node predict false, while right implies true.

\section*{15}

$E_{in} = 0.0, E_{out} = 0.126$.

\section*{16}

Purining three leaves results in same lowest $E_{in} = 0.01$, while the corresponding $E_{out} = 0.144, E_{out} = 0.117, E_{out} = 0.109$.

\section*{17}

\begin{align*}
  U_{t+1}
  =& \sum_{i=1}^N u_i^{(t + 1)} \\
  =& \sum_{i=1}^N
     \begin{cases}
       u_i^{(t)} \cdot \sqrt{\frac{1 - \epsilon_t}{\epsilon_t}} & g_t(x_i) \ne y_i\\
       u_i^{(t)} / \sqrt{\frac{1 - \epsilon_t}{\epsilon_t}} & g_t(x_i) = y_i\\
     \end{cases}
  \\
  =& \sqrt{\frac{1 - \epsilon_t}{\epsilon_t}} \sum_{\{i | g_t(x_i) \ne y_i \}} u_i^{(t)} +
     \sqrt{\frac{\epsilon_t}{1 - \epsilon_t}} \sum_{\{i | g_t(x_i) = y_i \}} u_i^{(t)} \\
  =& \sqrt{\frac{1 - \epsilon_t}{\epsilon_t}} \cdot \epsilon_t U_t +
     \sqrt{\frac{\epsilon_t}{1 - \epsilon_t}} \cdot (1 - \epsilon) U_t \\
  =& U_t \cdot 2 \sqrt{\epsilon_t (1 - \epsilon_t )} \\
  \leq& U_t \cdot 2 \sqrt{\epsilon(1 - \epsilon)}
\end{align*}

The last $\leq$ is because

\begin{align*}
  &  \epsilon(1 - \epsilon) - \epsilon_t (1 - \epsilon_t ) \\
  =& \epsilon - \epsilon^2 - \epsilon_t + \epsilon_t^2 \\
  =& (\epsilon_t + \epsilon)(\epsilon_t - \epsilon) + \epsilon - \epsilon_t \\
  =& (\epsilon_t + \epsilon - 1)(\epsilon_t - \epsilon) \geq 0
   & \because \epsilon_t + \epsilon \leq 1; \epsilon_t \leq \epsilon\\
  \implies& \epsilon_t(1 - \epsilon_t) \leq \epsilon (1 - \epsilon )
\end{align*}

\section*{18}

\begin{align*}
  E_{in}(G_{t+2}) \leq U_{t+1}
  =& U_t \cdot 2 \sqrt{\epsilon_t(1 - \epsilon_t)} \\
  \leq& U_t \cdot 2 \sqrt{\epsilon(1 - \epsilon)} \\
  \leq& U_t \exp(-2 (\frac{1}{2} - \epsilon)^2 )
\end{align*}

Since $\exp(-2 (\frac{1}{2} - \epsilon)^2 ) > 1$, after $T = \lceil \log{2N} / \log{\exp(-2 (\frac{1}{2} - \epsilon)^2 )} \rceil$ iterations,

\begin{align*}
  E_{in}(G_{T - 1}) \leq U_{T} 
  \leq& \exp(-2 (\frac{1}{2} - \epsilon)^2 )^{-T} U_1 \\
  \leq& \frac{1}{2N} U_1 \\
  \leq& \frac{1}{2N} \\
  =& 0 & \because \text{ $E_{in} \in \{ 0, \frac{1}{N}, \frac{2}{N}, \cdots, 1$ \}}
\end{align*}

Therefore, after $T = O(\log(N))$ iterations, $E_{in}(G_T) = 0$.

\end{document}
